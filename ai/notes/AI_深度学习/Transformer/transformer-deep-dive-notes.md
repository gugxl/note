# Transformer ä»å…¥é—¨åˆ°å…¥åœŸç¬”è®°

> **åŸæ–‡**: [æƒ³å¸®ä½ å¿«é€Ÿå…¥é—¨è§†è§‰Transformerï¼Œä¸€ä¸å°å¿ƒå†™äº†3Wå­—](https://mp.weixin.qq.com/s/7MjBJlczIxElTDrMh7yriQ)  
> **æ•´ç†æ—¶é—´**: 2026-02-14  
> **éš¾åº¦**: è¿›é˜¶ â­â­â­

---

## ğŸ“Œ ä¸€å¥è¯æ€»ç»“

æœ¬æ–‡ä»**é›¶å¼€å§‹**è¯¦ç»†è®²è§£äº†Transformerçš„åŸç†å’Œä»£ç å®ç°ï¼Œå¹¶æ·±å…¥åˆ†æäº†å…¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸¤å¤§åº”ç”¨ï¼š**Vision Transformerï¼ˆå›¾åƒåˆ†ç±»ï¼‰**å’Œ**DETRï¼ˆç›®æ ‡æ£€æµ‹ï¼‰**ã€‚

---

## ğŸ¯ ä¸ºä»€ä¹ˆè¦å­¦Transformerï¼Ÿ

```mermaid
graph LR
    A[2017å¹´å‰] -->|RNN/LSTM| B[é¡ºåºè®¡ç®—<br/>æ…¢ä¸”ä¿¡æ¯æ˜“ä¸¢å¤±]
    C[2017å¹´å] -->|Transformer| D[å¹¶è¡Œè®¡ç®—<br/>å…¨å±€æ³¨æ„åŠ›]
    B --> E[é•¿æ–‡æœ¬/é•¿è·ç¦»<br/>éš¾ä»¥å»ºæ¨¡]
    D --> F[ä»»æ„è·ç¦»å…³ç³»<br/>ç›´æ¥æ•æ‰]
    
    style C fill:#e1f5e1,stroke:#333
    style F fill:#fff3cd,stroke:#333
```

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
- âœ… **å¹¶è¡Œè®¡ç®—** - ä¸åƒRNNå¿…é¡»ä¸€ä¸ªä¸€ä¸ªç®—
- âœ… **é•¿è·ç¦»ä¾èµ–** - å¥å­å¼€å¤´å’Œç»“å°¾çš„å…³ç³»å¯ä»¥ç›´æ¥å»ºæ¨¡
- âœ… **æ€§èƒ½å“è¶Š** - æˆä¸ºNLPå’ŒCVé¢†åŸŸçš„æ ‡å‡†æ¶æ„

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šTransformeråŸºç¡€

### 1.1 ä»Seq2Seqåˆ°Transformerçš„æ¼”è¿›

#### æ—©æœŸSeq2Seqï¼ˆRNNæ—¶ä»£ï¼‰

```mermaid
graph TB
    subgraph "Encoder-Decoder ç»“æ„"
        A[è¾“å…¥: Hello World] --> B[Encoder<br/>RNN/LSTM]
        B --> C[ä¸Šä¸‹æ–‡å‘é‡<br/>å›ºå®šé•¿åº¦]
        C --> D[Decoder<br/>RNN/LSTM]
        D --> E[è¾“å‡º: ä½ å¥½ä¸–ç•Œ]
    end
    
    Note["âŒ é—®é¢˜: ä¸Šä¸‹æ–‡å‘é‡é•¿åº¦å›ºå®š<br/>é•¿å¥å­ä¿¡æ¯ä¼šä¸¢å¤±"]
```

**å­˜åœ¨çš„é—®é¢˜ï¼š**
1. æ— è®ºè¾“å…¥å¤šé•¿ï¼Œéƒ½å‹ç¼©æˆä¸€ä¸ªå›ºå®šå‘é‡ â†’ **ä¿¡æ¯ç“¶é¢ˆ**
2. åªèƒ½é¡ºåºè®¡ç®—ï¼Œæ— æ³•å¹¶è¡Œ â†’ **è®­ç»ƒæ…¢**
3. é•¿è·ç¦»ä¾èµ–å›°éš¾ â†’ **æ¢¯åº¦æ¶ˆå¤±**

#### åŠ å…¥Attentionçš„Seq2Seq

```mermaid
graph TB
    subgraph "å¸¦Attentionçš„è§£ç "
        A[è§£ç ç¬¬3ä¸ªå­—] --> B{Query: è¦ç”Ÿæˆä»€ä¹ˆ?}
        B --> C[ä¸ç¼–ç å™¨<br/>æ‰€æœ‰è¾“å‡ºè®¡ç®—ç›¸ä¼¼åº¦]
        C --> D[åŠ æƒæ±‚å’Œ<br/>å¾—åˆ°ä¸Šä¸‹æ–‡]
        D --> E[ç”Ÿæˆç¬¬3ä¸ªå­—]
    end
    
    Note["âœ… æ”¹è¿›: è§£ç æ—¶åŠ¨æ€å…³æ³¨ç¼–ç å™¨çš„ä¸åŒä½ç½®"]
```

#### Transformerï¼ˆAttention is All You Needï¼‰

```mermaid
graph TB
    A[Input] --> B[Embedding<br/>+ ä½ç½®ç¼–ç ]
    B --> C[Encoder x6]
    C --> D[Decoder x6]
    D --> E[Output]
    
    subgraph "Encoderç»“æ„"
        F[Multi-Head<br/>Attention] --> G[Add&Norm]
        G --> H[Feed Forward] --> I[Add&Norm]
    end
    
    subgraph "Decoderç»“æ„"
        J[Masked<br/>Multi-Head<br/>Attention] --> K[Add&Norm]
        K --> L[Cross<br/>Attention] --> M[Add&Norm]
        M --> N[Feed Forward] --> O[Add&Norm]
    end
```

**æ ¸å¿ƒåˆ›æ–°ï¼š**
- å®Œå…¨æŠ›å¼ƒRNN/CNNï¼Œ**åªç”¨Attention**
- ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå„6å±‚
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- ä½ç½®ç¼–ç 

---

### 1.2 Self-Attention è¯¦è§£ï¼ˆæ ¸å¿ƒï¼ï¼‰

#### ä»€ä¹ˆæ˜¯Self-Attentionï¼Ÿ

```mermaid
graph LR
    A[è¾“å…¥å¥å­] --> B["The animal didn't cross<br/>the street because it<br/>was too tired"]
    B --> C[it æŒ‡çš„æ˜¯ä»€ä¹ˆ?]
    C --> D[animal] 
    C --> E[street]
    
    D -->|ç›¸å…³åº¦0.95| F[Self-Attention<br/>è‡ªåŠ¨å‘ç°å…³ç³»]
    E -->|ç›¸å…³åº¦0.1| F
    
    style D fill:#90EE90,stroke:#333
    style E fill:#ffcccc,stroke:#333
```

#### è®¡ç®—æµç¨‹ï¼ˆä¸‰æ­¥èµ°ï¼‰

```mermaid
graph TB
    subgraph "Step 1: ç”ŸæˆQã€Kã€V"
        A[è¾“å…¥X<br/>shape: (seq_len, d_model)] --> B[ä¹˜ä»¥W_Q<br/>å¾—åˆ°Q]
        A --> C[ä¹˜ä»¥W_K<br/>å¾—åˆ°K]
        A --> D[ä¹˜ä»¥W_V<br/>å¾—åˆ°V]
        
        B --> E[Q: Query<br/>æˆ‘è¦æŸ¥ä»€ä¹ˆ]
        C --> F[K: Key<br/>æˆ‘æ˜¯è°]
        D --> G[V: Value<br/>æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯]
    end
    
    subgraph "Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°"
        E --> H[Q Ã— K^T<br/>ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦]
        H --> I[é™¤ä»¥âˆšd_k<br/>é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸]
        I --> J[Softmax<br/>è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ]
    end
    
    subgraph "Step 3: åŠ æƒæ±‚å’Œ"
        J --> K[ä¹˜ä»¥V<br/>æ³¨æ„åŠ›Ã—å€¼]
        K --> L[è¾“å‡ºZ<br/>åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯]
    end
```

**å…¬å¼è¡¨ç¤ºï¼š**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

**çŸ©é˜µå½¢å¼ï¼š**
```
è¾“å…¥X: (seq_len, d_model)
W_Q, W_K, W_V: (d_model, d_k)

Q = X Ã— W_Q  â†’  (seq_len, d_k)
K = X Ã— W_K  â†’  (seq_len, d_k)  
V = X Ã— W_V  â†’  (seq_len, d_v)

Attention = softmax(Q Ã— K^T / âˆšd_k) Ã— V
           â†’  (seq_len, seq_len) Ã— (seq_len, d_v)
           â†’  (seq_len, d_v)
```

---

### 1.3 Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

#### ä¸ºä»€ä¹ˆè¦å¤šå¤´ï¼Ÿ

```mermaid
graph TB
    A[ä¸€å¥è¯] --> B[å¤´1: è¯­æ³•å…³ç³»<br/>ä¸»è°“å®¾]
    A --> C[å¤´2: è¯­ä¹‰å…³ç³»<br/>åŒä¹‰è¯]
    A --> D[å¤´3: æŒ‡ä»£å…³ç³»<br/>it=animal]
    A --> E[å¤´4-8: å…¶ä»–å…³ç³»]
    
    B --> F[æ‹¼æ¥ç»“æœ]
    C --> F
    D --> F
    E --> F
    
    F --> G[ç»¼åˆç†è§£<br/>æ›´å…¨é¢]
    
    style F fill:#90EE90,stroke:#333
```

#### å¤šå¤´è®¡ç®—è¿‡ç¨‹

```mermaid
graph LR
    A[è¾“å…¥X<br/>512ç»´] --> B{åˆ†æˆ8ä¸ªå¤´}
    
    B --> C[å¤´1<br/>64ç»´]
    B --> D[å¤´2<br/>64ç»´]
    B --> E[...]
    B --> F[å¤´8<br/>64ç»´]
    
    C --> G[Attentionè®¡ç®—]
    D --> G
    E --> G
    F --> G
    
    G --> H[8ä¸ªç»“æœ<br/>å„64ç»´]
    H --> I[Concatæ‹¼æ¥<br/>512ç»´]
    I --> J[Linearå˜æ¢<br/>è¾“å‡º]
    
    style I fill:#e1f5e1,stroke:#333
```

**ä»£ç å®ç°ï¼š**
```python
# ä¼ªä»£ç ç¤ºæ„
class MultiHeadAttention:
    def __init__(self, d_model=512, n_heads=8):
        self.d_k = d_model // n_heads  # 64
        self.n_heads = n_heads
        
        # 8ä¸ªå¤´çš„W_Q, W_K, W_V
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)  
        self.W_V = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        # x: (batch, seq_len, 512)
        
        # 1. ç”ŸæˆQKV
        Q = self.W_Q(x)  # (batch, seq_len, 512)
        K = self.W_K(x)
        V = self.W_V(x)
        
        # 2. åˆ†æˆ8ä¸ªå¤´
        Q = Q.view(batch, seq_len, 8, 64).transpose(1, 2)
        # Q: (batch, 8, seq_len, 64)
        
        # 3. æ¯ä¸ªå¤´å•ç‹¬è®¡ç®—Attention
        scores = Q @ K.transpose(-2, -1) / sqrt(64)
        attn = softmax(scores, dim=-1)
        out = attn @ V  # (batch, 8, seq_len, 64)
        
        # 4. æ‹¼æ¥8ä¸ªå¤´
        out = out.transpose(1, 2).contiguous()
        out = out.view(batch, seq_len, 512)
        
        return out
```

---

### 1.4 ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

#### ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

```mermaid
graph TB
    A[Transformeré—®é¢˜] --> B[åŒæ—¶çœ‹æ‰€æœ‰è¯<br/>ä¸çŸ¥é“é¡ºåº]
    B --> C["æˆ‘æ‰“ä½ "]
    B --> D["ä½ æ‰“æˆ‘"]
    C --> E[Transformerçœ¼ä¸­<br/>éƒ½æ˜¯ æ‰“/æˆ‘/ä½ ]
    D --> E
    
    E --> F[åˆ†ä¸æ¸…è¯­åº!]
    
    style F fill:#ff9999,stroke:#333
```

#### ä½ç½®ç¼–ç çš„è§£å†³æ–¹æ¡ˆ

```mermaid
graph TB
    subgraph "ç»™æ¯ä¸ªè¯åŠ ä¸ªåœ°å€æ ‡ç­¾"
        A1[æˆ‘+ä½ç½®1] --- A2[æ‰“+ä½ç½®2] --- A3[ä½ +ä½ç½®3]
    end
    
    subgraph "ä¸åŒä½ç½®æœ‰ä¸åŒçš„ç¼–ç å‘é‡"
        B1[ä½ç½®1: 0.0, 1.0, 0.0...] 
        B2[ä½ç½®2: 0.8, 0.9, 0.1...]
        B3[ä½ç½®3: 0.3, 0.2, 0.7...]
    end
    
    A1 -.-> B1
    A2 -.-> B2
    A3 -.-> B3
    
    C[æ¨¡å‹çŸ¥é“:<br/>ä½ç½®1åœ¨å‰ï¼Œä½ç½®3åœ¨å]
```

#### æ­£å¼¦ä½ç½®ç¼–ç å…¬å¼

```python
# è®ºæ–‡ä¸­çš„æ­£å¼¦ç¼–ç 
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

# pos: è¯çš„ä½ç½®(0, 1, 2, ...)
# i:   ç»´åº¦ç´¢å¼•(0, 1, 2, ..., d_model-1)
```

**å¯è§†åŒ–ç†è§£ï¼š**

```
ä½ç½®ç¼–ç çŸ©é˜µ (pos Ã— dim):

       dim0   dim1   dim2   dim3   ...  dim511
pos0   sin    cos    sin    cos   ...   cos
pos1   sin    cos    sin    cos   ...   cos  
pos2   sin    cos    sin    cos   ...   cos
...    ...    ...    ...    ...   ...   ...
pos99  sin    cos    sin    cos   ...   cos

æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä½ç½®çš„ç¼–ç 
æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªæ­£å¼¦/ä½™å¼¦æ³¢ï¼Œé¢‘ç‡ä¸åŒ
```

**ä¼˜ç‚¹ï¼š**
- å¯ä»¥æ‰©å±•åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åº¦
- ç›¸å¯¹ä½ç½®å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°
- æœ‰å‘¨æœŸæ€§è§„å¾‹

---

### 1.5 Transformerå®Œæ•´æ¶æ„

#### ç¼–ç å™¨ï¼ˆEncoderï¼‰

```mermaid
graph TB
    subgraph "ä¸€å±‚Encoder"
        A[è¾“å…¥] --> B[Multi-Head<br/>Self-Attention]
        B --> C[Add & Norm<br/>æ®‹å·®+å±‚å½’ä¸€åŒ–]
        C --> D[Feed Forward<br/>å‰é¦ˆç½‘ç»œ]
        D --> E[Add & Norm<br/>æ®‹å·®+å±‚å½’ä¸€åŒ–]
        E --> F[è¾“å‡º]
        
        A -.->|æ®‹å·®è¿æ¥| C
        C -.->|æ®‹å·®è¿æ¥| E
    end
    
    Note["é‡å¤N=6å±‚"]
```

**å„ç»„ä»¶è¯´æ˜ï¼š**

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| **Multi-Head Self-Attention** | è®¡ç®—è¯ä¸è¯ä¹‹é—´çš„å…³ç³» |
| **Add & Norm** | æ®‹å·®è¿æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œå±‚å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ |
| **Feed Forward** | ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼Œå¢åŠ éçº¿æ€§è¡¨è¾¾èƒ½åŠ› |

#### è§£ç å™¨ï¼ˆDecoderï¼‰

```mermaid
graph TB
    subgraph "ä¸€å±‚Decoder"
        A[è¾“å…¥] --> B[Masked<br/>Multi-Head<br/>Self-Attention]
        B --> C[Add & Norm]
        C --> D[Cross Attention<br/>Qæ¥è‡ªè§£ç å™¨<br/>KVæ¥è‡ªç¼–ç å™¨]
        D --> E[Add & Norm]
        E --> F[Feed Forward]
        F --> G[Add & Norm]
        G --> H[è¾“å‡º]
    end
```

**è§£ç å™¨ç‰¹æœ‰ç»„ä»¶ï¼š**

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| **Masked Self-Attention** | é˜²æ­¢çœ‹åˆ°æœªæ¥çš„è¯ï¼ˆåªèƒ½attendåˆ°å‰é¢çš„è¯ï¼‰ |
| **Cross Attention** | è§£ç å™¨æŸ¥è¯¢(Query)å…³æ³¨ç¼–ç å™¨çš„è¾“å‡º(Key, Value) |

#### Masked Self-Attention è¯¦è§£

```mermaid
graph LR
    A[è§£ç ç¬¬3ä¸ªå­—æ—¶<br/>åªèƒ½çœ‹å‰3ä¸ªå­—] --> B[MaskçŸ©é˜µ]
    
    B --> C["ä½ç½®1: [1, 0, 0, 0, 0]<br/>åªèƒ½çœ‹è‡ªå·±"]
    B --> D["ä½ç½®2: [1, 1, 0, 0, 0]<br/>èƒ½çœ‹å‰2ä¸ª"]
    B --> E["ä½ç½®3: [1, 1, 1, 0, 0]<br/>èƒ½çœ‹å‰3ä¸ª"]
    B --> F["ä½ç½®4: [1, 1, 1, 1, 0]<br/>èƒ½çœ‹å‰4ä¸ª"]
    
    Note["ä¸‹ä¸‰è§’çŸ©é˜µ<br/>softmaxå‰æŠŠä¸Šä¸‰è§’è®¾ä¸º-âˆ"]
```

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šè§†è§‰Transformer

### 2.1 Vision Transformer (ViT)

#### æ ¸å¿ƒæ€æƒ³

```mermaid
graph TB
    A[å›¾ç‰‡224Ã—224] --> B[åˆ‡æˆ16Ã—16çš„patch<br/>14Ã—14=196ä¸ª]
    B --> C[æ¯ä¸ªpatchæ‹‰å¹³<br/>16Ã—16Ã—3=768]
    C --> D[LinearæŠ•å½±<br/>768â†’512]
    D --> E[åŠ CLS Token<br/>+ä½ç½®ç¼–ç ]
    E --> F["è¾“å…¥Transformer<br/>(196+1)ä¸ªtoken"]
    F --> G[å–CLSè¾“å‡º<br/>åˆ†ç±»]
    
    style G fill:#90EE90,stroke:#333
```

**å…³é”®æ­¥éª¤ï¼š**

#### Step 1: å›¾ç‰‡åˆ†å—ï¼ˆPatch Embeddingï¼‰

```python
# è¾“å…¥: (batch, 3, 224, 224)
# patch_size = 16

# åˆ‡åˆ†æˆ 14Ã—14 = 196 ä¸ª patch
x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', 
              p1=16, p2=16)
# è¾“å‡º: (batch, 196, 768)
# 196 = 14Ã—14
# 768 = 16Ã—16Ã—3 (æ¯ä¸ªpatchçš„åƒç´ æ•°)

# é™ç»´åˆ°æ¨¡å‹ç»´åº¦
x = nn.Linear(768, 512)(x)
# è¾“å‡º: (batch, 196, 512)
```

#### Step 2: æ·»åŠ CLS Token

```mermaid
graph LR
    A[196ä¸ªpatch<br/>196Ã—512] --> B[åŠ 1ä¸ªCLS Token]
    B --> C[197ä¸ªtoken<br/>197Ã—512]
    
    Note["CLS Tokenä½œç”¨:<br/>èšåˆæ•´å¼ å›¾çš„ä¿¡æ¯<br/>ç”¨äºåˆ†ç±»"]
```

```python
# å¯å­¦ä¹ çš„CLS Token
self.cls_token = nn.Parameter(torch.randn(1, 1, 512))

# å¤åˆ¶åˆ°batchç»´åº¦
cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=batch_size)

# æ‹¼æ¥åˆ°æœ€å‰é¢
x = torch.cat([cls_tokens, x], dim=1)
# è¾“å‡º: (batch, 197, 512)
```

#### Step 3: ä½ç½®ç¼–ç 

```python
# å¯å­¦ä¹ çš„ä½ç½®ç¼–ç 
self.pos_embedding = nn.Parameter(torch.randn(1, 197, 512))

# åŠ ä¸Šä½ç½®ä¿¡æ¯
x = x + self.pos_embedding
```

#### Step 4: Transformerç¼–ç å™¨

```python
# å’Œæ ‡å‡†Transformerä¸€æ ·
for _ in range(12):  # 12å±‚
    x = MultiHeadAttention(x)
    x = FeedForward(x)
    
# è¾“å‡º: (batch, 197, 512)
```

#### Step 5: åˆ†ç±»å¤´

```python
# åªå–CLS Tokençš„è¾“å‡º
cls_output = x[:, 0]  # (batch, 512)

# åˆ†ç±»
output = nn.Linear(512, num_classes)(cls_output)
# è¾“å‡º: (batch, num_classes)
```

---

### 2.2 DETRï¼ˆç›®æ ‡æ£€æµ‹ï¼‰

#### æ ¸å¿ƒæ€æƒ³

```mermaid
graph TB
    A[ä¼ ç»Ÿæ£€æµ‹<br/>Faster R-CNN] --> B[Anchor â†’ NMS<br/>å¤æ‚åå¤„ç†]
    C[DETR] --> D[ç«¯åˆ°ç«¯<br/>é›†åˆé¢„æµ‹]
    
    B --> E[æ‰‹å·¥è®¾è®¡å¤š<br/>è°ƒå‚å›°éš¾]
    D --> F[ç›´æ¥è¾“å‡º<br/>100ä¸ªæ£€æµ‹æ¡†]
    
    style F fill:#90EE90,stroke:#333
```

**DETRçš„åˆ›æ–°ï¼š**
- æŠ›å¼ƒAnchorå’ŒNMS
- å°†æ£€æµ‹è§†ä¸º**é›†åˆé¢„æµ‹é—®é¢˜**
- å›ºå®šè¾“å‡º100ä¸ªæ£€æµ‹ç»“æœï¼ˆä¸å¤Ÿå°±ç”¨"æ— ç‰©ä½“"å¡«å……ï¼‰

#### æ•´ä½“æ¶æ„

```mermaid
graph TB
    A[è¾“å…¥å›¾ç‰‡] --> B[CNN Backbone<br/>ResNet50]
    B --> C[ç‰¹å¾å›¾<br/>H/32 Ã— W/32 Ã— 2048]
    C --> D[1Ã—1 Convé™ç»´<br/>â†’256ç»´]
    D --> E[æ‹‰å¹³æˆåºåˆ—<br/>è¾“å…¥Transformer]
    
    subgraph "Transformer"
        F[Encoder] --> G[Decoder]
        H[Object Queries<br/>100ä¸ªå¯å­¦ä¹ å‘é‡] --> G
    end
    
    G --> I[100ä¸ªæ£€æµ‹ç»“æœ<br/>æ¯ä¸ª: ç±»åˆ«+åæ ‡]
    
    style H fill:#fff3cd,stroke:#333
```

#### å…³é”®ç»„ä»¶

##### 1. Object Queriesï¼ˆå¯¹è±¡æŸ¥è¯¢ï¼‰

```mermaid
graph TB
    A[100ä¸ªObject Queries] --> B[Query1: æ‰¾æœ€å¤§çš„ç‰©ä½“]
    A --> C[Query2: æ‰¾å·¦ä¸Šè§’çš„ç‰©ä½“]
    A --> D[Query3: æ‰¾çº¢è‰²çš„ç‰©ä½“]
    A --> E[...]
    
    B --> F[è§£ç å™¨è®¡ç®—]
    C --> F
    D --> F
    E --> F
    
    F --> G[è¾“å‡º100ä¸ª<br/>(ç±»åˆ«, bbox)]
```

**å®ç°ï¼š**
```python
# 100ä¸ªå¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡
self.query_embed = nn.Embedding(100, 256)
```

##### 2. åŒè¾¹åŒ¹é…ï¼ˆBipartite Matchingï¼‰

**é—®é¢˜ï¼š** è¾“å‡ºçš„100ä¸ªæ£€æµ‹ç»“æœæ˜¯æ— åºçš„ï¼Œå¦‚ä½•å’ŒGround TruthåŒ¹é…ï¼Ÿ

```mermaid
graph TB
    A[é¢„æµ‹: 100ä¸ªæ¡†] --> B[åŒˆç‰™åˆ©ç®—æ³•<br/>æœ€ä¼˜åŒ¹é…]
    C[GT: Mä¸ªæ¡†<br/>Mâ‰¤100] --> B
    
    B --> D[æ‰¾åˆ°æœ€ä½³åŒ¹é…å¯¹<br/>æœ€å°åŒ–æ€»ä½“æŸå¤±]
    
    E["æŸå¤±åŒ…æ‹¬:<br/>- åˆ†ç±»æŸå¤±<br/>- L1åæ ‡æŸå¤±<br/>- GIoUæŸå¤±"]
```

**ä»£ç ç¤ºæ„ï¼š**
```python
from scipy.optimize import linear_sum_assignment

# è®¡ç®—100ä¸ªé¢„æµ‹å’ŒMä¸ªGTä¹‹é—´çš„åŒ¹é…ä»£ä»·
cost = cost_class + cost_bbox + cost_giou  # (100, M)

# åŒˆç‰™åˆ©ç®—æ³•æ±‚è§£æœ€ä¼˜åŒ¹é…
indices = linear_sum_assignment(cost)
# è¿”å›åŒ¹é…çš„ç´¢å¼•å¯¹
```

##### 3. Lossè®¡ç®—

```python
def detr_loss(outputs, targets):
    # outputs: (batch, 100, 92) åˆ†ç±»
    #          (batch, 100, 4)  åæ ‡
    
    # 1. åŒˆç‰™åˆ©åŒ¹é…
    indices = hungarian_matching(outputs, targets)
    
    # 2. åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
    loss_cls = F.cross_entropy(pred_classes, target_classes)
    
    # 3. è¾¹ç•Œæ¡†æŸå¤±ï¼ˆL1 + GIoUï¼‰
    loss_bbox = F.l1_loss(pred_boxes, target_boxes)
    loss_giou = 1 - generalized_box_iou(pred_boxes, target_boxes)
    
    return loss_cls + loss_bbox + loss_giou
```

---

## ğŸ” å…³é”®ä»£ç è§£æ

### 3.1 Scaled Dot-Product Attention

```python
class ScaledDotProductAttention(nn.Module):
    def forward(self, q, k, v, mask=None):
        # q, k, v: (batch, heads, seq_len, d_k)
        
        # 1. è®¡ç®—ç›¸ä¼¼åº¦
        scores = torch.matmul(q, k.transpose(-2, -1)) / sqrt(d_k)
        # (batch, heads, seq_len, seq_len)
        
        # 2. Maskï¼ˆè§£ç å™¨ç”¨ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 3. Softmax
        attn = F.softmax(scores, dim=-1)
        
        # 4. åŠ æƒæ±‚å’Œ
        output = torch.matmul(attn, v)
        # (batch, heads, seq_len, d_k)
        
        return output, attn
```

### 3.2 ViTå®Œæ•´ä»£ç ç»“æ„

```python
class ViT(nn.Module):
    def __init__(self, image_size=224, patch_size=16, num_classes=1000, 
                 dim=512, depth=12, heads=8):
        super().__init__()
        
        # 1. Patch Embedding
        self.patch_size = patch_size
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size ** 2
        self.patch_to_embedding = nn.Linear(patch_dim, dim)
        
        # 2. CLS Token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
        # 3. ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        
        # 4. Transformer
        self.transformer = Transformer(dim, depth, heads)
        
        # 5. åˆ†ç±»å¤´
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )
    
    def forward(self, img):
        b = img.shape[0]
        
        # åˆ‡patch
        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', 
                      p1=self.patch_size, p2=self.patch_size)
        
        # Embedding
        x = self.patch_to_embedding(x)
        
        # åŠ CLS
        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # åŠ ä½ç½®ç¼–ç 
        x += self.pos_embedding
        
        # Transformer
        x = self.transformer(x)
        
        # åˆ†ç±»
        return self.mlp_head(x[:, 0])
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### Transformer vs CNN

| æŒ‡æ ‡ | CNN (ResNet) | Transformer (ViT) |
|------|-------------|-------------------|
| **æ•°æ®æ•ˆç‡** | é«˜ï¼ˆImageNetå³å¯ï¼‰ | ä½ï¼ˆéœ€è¦JFT-300Mï¼‰ |
| **è®¡ç®—é‡** | å° | å¤§ï¼ˆçº¦30å€ï¼‰ |
| **å…¨å±€æ„ŸçŸ¥** | âŒ éœ€å †å å±‚æ•° | âœ… ä¸€å±‚å³å¯ |
| **å¯è§£é‡Šæ€§** | ä¸­ç­‰ | é«˜ï¼ˆAttentionå¯è§†åŒ–ï¼‰ |
| **æ‰©å±•æ€§** | æœ‰é™ | å¼ºï¼ˆScaleæ•ˆæœå¥½ï¼‰ |

### è®­ç»ƒæ•°æ®å½±å“

```mermaid
graph LR
    A[ImageNet-1k<br/>130ä¸‡å¼ ] -->|ViTæ€§èƒ½å·®| B[CNNæ›´å¥½]
    C[ImageNet-21k<br/>1400ä¸‡å¼ ] -->|ViTè¿½å¹³| D[ç›¸å½“]
    E[JFT-300M<br/>3äº¿å¼ ] -->|ViTè¶…è¶Š| F[SOTA]
    
    style F fill:#90EE90,stroke:#333
```

**ç»“è®ºï¼š** Transformeræ˜¯**æ•°æ®é¥¥æ¸´å‹**æ¨¡å‹ï¼Œæ•°æ®é‡è¶Šå¤§ä¼˜åŠ¿è¶Šæ˜æ˜¾ã€‚

---

## ğŸš€ å‘å±•è¶‹åŠ¿

```mermaid
timeline
    title Transformerå‘å±•å†ç¨‹
    2017 : Transformerè¯ç”Ÿ
         : Attention is All You Need
    2020 : ViTå‘å¸ƒ
         : è§†è§‰Transformerå…ƒå¹´
    2021 : DeiT/Swinå‘å¸ƒ
         : æ•°æ®æ•ˆç‡ä¼˜åŒ–
    2022 : CLIP/DALL-E
         : å¤šæ¨¡æ€çˆ†å‘
    2023 : GPT-4V/SAM
         : å¤§æ¨¡å‹æ—¶ä»£
    2024 : å‡ ä¹ç»Ÿæ²»CVæ‰€æœ‰ä»»åŠ¡
```

---

## ğŸ’¡ å­¦ä¹ å»ºè®®

### å…¥é—¨è·¯çº¿

1. **ç†è§£Self-Attention**ï¼ˆæœ€é‡è¦ï¼ï¼‰
   - æ‰‹åŠ¨ç”»ä¸€éè®¡ç®—æµç¨‹
   - ç†è§£Qã€Kã€Vçš„å«ä¹‰

2. **é˜…è¯»PyTorchå®˜æ–¹å®ç°**
   - `nn.MultiheadAttention`
   - `nn.Transformer`

3. **åŠ¨æ‰‹å®ç°**
   - å…ˆå®ç°ä¸€ä¸ªç®€å•çš„Transformer
   - å†å®ç°ViT
   - æœ€åç”¨é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ

4. **é˜…è¯»ç»å…¸è®ºæ–‡**
   - Attention is All You Need
   - ViT
   - DETR

### æ¨èèµ„æº

| ç±»å‹ | èµ„æº | é“¾æ¥ |
|------|------|------|
| **è®ºæ–‡** | Attention is All You Need | arxiv.org/abs/1706.03762 |
| **è®ºæ–‡** | ViT | arxiv.org/abs/2010.11929 |
| **è®ºæ–‡** | DETR | arxiv.org/abs/2005.12872 |
| **ä»£ç ** | ViT PyTorch | github.com/lucidrains/vit-pytorch |
| **ä»£ç ** | DETR | github.com/facebookresearch/detr |
| **æ•™ç¨‹** | The Illustrated Transformer | jalammar.github.io/illustrated-transformer |

---

## ğŸ¯ æ€»ç»“

### Transformeræ ¸å¿ƒè¦ç‚¹

```mermaid
graph TB
    A[Transformeræ ¸å¿ƒ] --> B[Self-Attention<br/>è®¡ç®—è¯ä¸è¯å…³ç³»]
    A --> C[Multi-Head<br/>å¤šè§’åº¦ç†è§£]
    A --> D[Positional Encoding<br/>ä½ç½®ä¿¡æ¯]
    A --> E[Residual+Norm<br/>è®­ç»ƒç¨³å®š]
    
    B --> F[ä»»æ„è·ç¦»å»ºæ¨¡]
    C --> G[å¤šè¯­ä¹‰æ•æ‰]
    D --> H[åºåˆ—é¡ºåº]
    E --> I[æ·±å±‚è®­ç»ƒ]
```

### è§†è§‰é¢†åŸŸåº”ç”¨

| ä»»åŠ¡ | æ–¹æ³• | å…³é”®åˆ›æ–° |
|------|------|----------|
| **åˆ†ç±»** | ViT | Patch + CLS Token |
| **æ£€æµ‹** | DETR | Object Query + åŒè¾¹åŒ¹é… |
| **åˆ†å‰²** | Segmenter | è§£ç å™¨ä¸Šé‡‡æ · |
| **å¤šæ¨¡æ€** | CLIP | å›¾æ–‡å¯¹é½ |

---

## ğŸ“š ç›¸å…³èµ„æº

- **åŸæ–‡**: [æƒ³å¸®ä½ å¿«é€Ÿå…¥é—¨è§†è§‰Transformerï¼Œä¸€ä¸å°å¿ƒå†™äº†3Wå­—](https://mp.weixin.qq.com/s/7MjBJlczIxElTDrMh7yriQ)
- **ä½œè€…**: æ·±åº¦çœ¸
- **æ¥æº**: AIç§‘æŠ€è¯„è®º

---

*æ•´ç†å®Œæˆï¼å¦‚æœ‰ç–‘é—®æ¬¢è¿ç»§ç»­æ¢è®¨ ğŸ‘‹*
