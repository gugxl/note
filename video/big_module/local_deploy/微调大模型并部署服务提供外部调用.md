@[TOC](微调大模型并部署服务提供外部调用)
应用到的技术 微调框架（LLama-Factory）
微调算法 LoRA （）
基础模型：
蒸馏模型：通常情况下把大模型的知识转移到小模型中，尽量保证模型的性能的同时减少参数和计算需求。
FastAPI：暴露Http接口给外部服务调用

## 1.背景知识介绍说明

一般情况下企业会有不同的个性化内容需要大模型进行适配， 就会对模型进行「对齐 / 优化 / 增强」常见的方式有：SFT(有监督微调)、RLHF（强化学习）、RAG（检索增强生成）

对比说明



| 方法                                                            | 核心思路                                    | 优点 | 缺点 | 典型场景                       |
|---------------------------------------------------------------|-----------------------------------------|----| ---- |----------------------------|
| SFT（Supervised Fine-Tuning） 有监督微调 | 用人工标注的数据集（输入→期望输出）对大模型做微调，让模型学习任务格式和知识。 | 实现简单，效果直观；适合让模型快速掌握领域任务。 | 需要高质量人工标注数据；只能学到数据里的知识，缺乏“价值观对齐”。 | 客服对话、SQL 生成、问答机器人、代码补全     |
| RLHF （Reinforcement Learning with Human Feedback） 基于人类反馈的强化学习 |先用 SFT 训练模型，再让模型生成多个答案，由人工或奖励模型打分，强化学习优化模型，使输出更符合人类偏好。| 输出更贴近人类价值观；能提升回答质量和安全性。 | 成本高（要人工打分/训练奖励模型）；训练复杂。| ChatGPT 类助手、对话安全优化、减少有害回答  |
| RAG（Retrieval-Augmented Generation）检索增强生成 | 在推理阶段，把外部知识库检索到的内容作为上下文提供给模型，再生成回答。| 不需要重新训练模型；知识可实时更新；解决模型“知识过时”问题。|性能依赖检索质量；上下文窗口有限；不会改变模型本身能力。| 企业知识库问答、法律/金融/医疗资料查询、文档助手  | 

SFT：主要是提高模型对企业专有信息的理解、增强模型对特定行业领域的学习。（垂直领域）
RLHF：提供个性化和互动性强的交互：会生成多个结果，由用户的反馈调整回答的方式
RAG： 将外部信息检索与⽂本⽣成结合，帮助模型在⽣成答案时，实时获取外部信息和最新信息

微调的细化又可以分为`有监督微调`和`自监督微调`
强化学习可以细化为 `DPO(Direct Preference Optimization)`通过使用者的**对比选择**,直接优化模型，调整幅度比较大[类似与A/B,选择A的结果或者B的结果]。`PPO(Proximal Policy Optimization)`通过`奖励信号`来渐进式调整模型的行为策略；调整幅度小。

微调
- 适合拥有非常充足的数据
- 能够直接提升模式的固有能力；无需依赖外部检索

RAG
- 适合只有很少的数据；动态更新的数据
- 每次回答问题前都需要耗时检索知识库；回答质量依赖检索系统的质量。

总而言之， 少量企业私有知识库：最好微调+RAG；资源不足优先RAG。需要动态更新数据：RAG。大量垂直领域知识：微调。

SFT（有监督微调）
通过**人工标注**的数据,进一步**训练预训练模型**，让模型能够更加精确的处理**特定领域**的任务
- 人工标注数据
  
例如：某些分类系统
```
{"image_path": "path/image1.jpg", "label": "SpongeBobSquarePants"}
{"image_path": "path/image2.jpg", "label": "PatrickStar"}
```

```json
{
"instruction": "请问你是谁",
"input": "",
"output": "您好，我是蟹堡王的神奇海螺，很⾼兴为您服务！我可以回答关于蟹堡王和汉堡制作的任何问题，您有什么需要帮助的吗？"
}
```
- 预训练模型（基座模型）
  指已经经过大量数据训练过的模型，也就是我们微调前需要预先下载的开源模型。它具备了较为通⽤的知识和能⼒，能够解决⼀些常⻅的任务，可以在此基础上进⾏进⼀步的微调 （`fine-tuning`）以适应特定的任务或领域
- 微调算法分类
  - 全参数微调（Full Fine-Tuning）：对整个模型进行微调，会更新所有参数
    - 优点：因为对每个参数都可以进行调整，可以得到最佳性能；能够适应不同任务和场景
    - 缺点：需要有大量的计算资源并且容易出现过拟合
  - 不分参数微调（Partial Fine-Tuning）：只更新模型中的部分参数，某些层或者某些模块。例如loRA
    - 优点:减少计算成本；减少过拟合风险；能以较小的代价获得较好的结果
    - 缺点：可能无法达到最佳性能



### LoRA 微调算法
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
介绍：提出了通过**低秩矩阵分解**的⽅式来进⾏**部分参数微调**，极⼤推动了 AI 技术在多⾏业的⼴泛落地应⽤

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
标志着 Transformer 架构的提出，彻底改变了⾃然语⾔处理（NLP）领域


## 2.


## 3.

## 4.

## 5.



`docker run -it --rm --gpus=all --ipc=host -p 7860:7860 -e HF_ENDPOINT=https://hf-mirror.com hiyouga/llamafactory:latest python -m src.llamafactory.cli webui --host 0.0.0.0 --port 7860`
